{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "c5f8ba3d-0d2d-448b-8f4e-759725a5bd52"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/env/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVR\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "MAX_TOWERS = 6\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "np.random.seed(17)\n",
    "import random\n",
    "random.seed(17)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "ed112cb9-b62f-4ea0-892e-430c7112d015"
    }
   },
   "outputs": [],
   "source": [
    "def get_train():\n",
    "    train_main = pd.read_csv(\"../data/task1/train_1.7.csv\", encoding=\"cp1251\")\n",
    "    train_aux_coords = pd.read_csv(\"../data/task1_additional/coords_train_1.1.csv\", encoding=\"cp1251\")\n",
    "    train_aux_frac = pd.read_csv(\"../data/task1_additional/frac_train_1.csv\", encoding=\"cp1251\")\n",
    "    train_aux_gdis = pd.read_csv(\"../data/task1_additional/gdis_train1.2.csv\", encoding=\"cp1251\")\n",
    "    \n",
    "    \n",
    "    train_frac_main = pd.merge(train_main, train_aux_frac,how=\"left\", left_on=\"Скважина\", right_on=\"Скважина\")\n",
    "    all_recs = pd.merge(train_frac_main, train_aux_gdis,how=\"left\", left_on=\"Скважина\", right_on=\"Скважина\")\n",
    "    print(all_recs.shape)\n",
    "    return all_recs\n",
    "\n",
    "def get_test():\n",
    "    test_main = pd.read_csv(\"../data/task1/test_1.9.csv\", encoding=\"cp1251\")\n",
    "    test_aux_coords = pd.read_csv(\"../data/task1_additional/coords_train_1.1.csv\", encoding=\"cp1251\")\n",
    "    test_aux_frac = pd.read_csv(\"../data/task1_additional/frac_test_1.csv\", encoding=\"cp1251\")\n",
    "    test_aux_gdis = pd.read_csv(\"../data/task1_additional/gdis_test1.2.csv\", encoding=\"cp1251\")\n",
    "    \n",
    "    \n",
    "    test_frac_main = pd.merge(test_main, test_aux_frac,how=\"left\", left_on=\"Скважина\", right_on=\"Скважина\")\n",
    "    all_recs = pd.merge(test_frac_main, test_aux_gdis,how=\"left\", left_on=\"Скважина\", right_on=\"Скважина\")\n",
    "    print(all_recs.shape)\n",
    "    return all_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by converted date and group\n",
    "def convert_and_sort(df):\n",
    "    df[\"Дата\"] =  df[\"Дата\"].apply(pd.to_datetime)\n",
    "    return df.sort_values(by=[\"Скважина\", \"Дата\"])\n",
    "\n",
    "def get_non_useful(df):\n",
    "    non_useful_columns = []\n",
    "    for c in df.columns:\n",
    "        null_columns = df[df[c].isnull()]\n",
    "        if len(null_columns)== len(df):\n",
    "            non_useful_columns.append(c)\n",
    "    return non_useful_columns\n",
    "\n",
    "def drop_non_useful(train, test):\n",
    "    non_useful = set(get_non_useful(train)) |set(get_non_useful(test))\n",
    "    print(\"%s dropped\"% non_useful)\n",
    "    return train.drop(list(non_useful), axis=1), test.drop(list(non_useful), axis=1)\n",
    "\n",
    "def get_float(v):\n",
    "    v = str(v)\n",
    "    if v != \"NaN\":\n",
    "        new = v.replace(\",\",\".\")\n",
    "        return float(new)\n",
    "    return v\n",
    "\n",
    "def get_target(df, column=\"Нефть, т\"):\n",
    "    target = df[column]\n",
    "    print(\"%s dropped\"% column)\n",
    "    return df.drop([column], axis=1), target.apply(get_float)\n",
    "\n",
    "#drop non present columns in test\n",
    "def drop_not_present(train, test):\n",
    "    absent_columns = list(set(train.columns) - set(test.columns))\n",
    "    print(\"%s dropped\"% absent_columns)\n",
    "    return train.drop(absent_columns, axis=1), test\n",
    "    \n",
    "def show_uniq_test_train(train, test):\n",
    "    #check all values that have zero ans nan only\n",
    "    for c in train.columns:\n",
    "        un = train[c].unique()\n",
    "        if len(un)<100:\n",
    "            tun = test[c].unique()\n",
    "            print(\"%s ;train: %s; test:%s\"%(c, un, tun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_data_pipeline(train, test):\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    \n",
    "    y = None\n",
    "    train=convert_and_sort(train)\n",
    "    train, test = drop_non_useful(train, test)\n",
    "    #remove target from train\n",
    "    train, y = get_target(train)\n",
    "    train, test = drop_not_present(train, test)\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    \n",
    "    return train, test, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "9b864954-91ac-40f5-b3c6-91c1c1518b4e"
    }
   },
   "outputs": [],
   "source": [
    "def get_existed(columns, df):\n",
    "    return list(set(columns)&set(df.columns))\n",
    "\n",
    "def split_continious_date_categorical_text(df):\n",
    "    group_id = [\"Скважина\"]\n",
    "    text = [\"Причина простоя\",\n",
    "            \"Куст\",\n",
    "            \"Состояние на конец месяца\",\n",
    "            \"Причина простоя.1\",\n",
    "            \"Мероприятия\",\n",
    "            \"Проппант\"]\n",
    "    categorical = [\"Тип испытания\",\n",
    "                   \"Тип скважины\",\n",
    "                   \"Неустановившийся режим\",\n",
    "                   \"ГТМ\",\n",
    "                   \"Метод\",\n",
    "                   \"Характер работы\",\n",
    "                   \"Состояние\",\n",
    "                   \"Пласт МЭР\", \n",
    "                   \"Способ эксплуатации\", \n",
    "                   \"Тип насоса\", \n",
    "                   \"Состояние на конец месяца\", \n",
    "                   \"Номер бригады\", \n",
    "                   \"Фонтан через насос\", \n",
    "                   \"Нерентабельная\",\n",
    "                   \"Назначение по проекту\",\n",
    "                   \"Группа фонда\",\n",
    "                   \"Тип дополнительного оборудования\",\n",
    "                   \"Марка ПЭД\",\n",
    "                   \"Тип ГЗУ\",\n",
    "                   \"ДНС\",\n",
    "                   \"КНС\",\n",
    "                   #useless potentially\n",
    "                   \"Диаметр плунжера\",\n",
    "                   \"Природный газ, м3\",\n",
    "                   \"Конденсат, т\",\n",
    "                   \"Длина хода плунжера ШГН\",\n",
    "                   \"Коэффициент подачи насоса\",\n",
    "                   \"Дебит конденсата\",\n",
    "                   \"Вязкость воды в пластовых условиях\",\n",
    "                   \"Газ из газовой шапки, м3\",\n",
    "                   \"Число качаний ШГН\",\n",
    "                   \"Коэффициент сепарации\",\n",
    "                   \"SKIN\",\n",
    "                   \"КН закрепленный\",\n",
    "                   # radically different\n",
    "                   \"Время в работе\",\n",
    "                   \"Радиус контура питания\",\n",
    "                   \"Время в накоплении\",\n",
    "                   \"Время накопления\",\n",
    "                   \"Агент закачки\"\n",
    "                   ]\n",
    "    dates = [\"Дата\", \n",
    "             \"Дата ГРП\",\n",
    "             \"Время до псевдоуст-ся режима\", \n",
    "             \"Дата запуска после КРС\", \n",
    "             \"Дата пуска\", \n",
    "             \"Дата останова\",\n",
    "             \"Дата ввода в эксплуатацию\"]\n",
    "    \n",
    "    continious = list(set(df.columns) - set(dates) - set(categorical) - set(text) - set(group_id))\n",
    "    return (df[group_id],df[continious], df[get_existed(dates,df)], df[get_existed(categorical, df)],\n",
    "            df[get_existed(text, df)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "20ee0dde-1884-4f1c-a0f9-35d3f1dd66f5"
    }
   },
   "outputs": [],
   "source": [
    "def get_object_columns(df):\n",
    "    objects = []\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype != pd.np.float:\n",
    "            objects.append(c)\n",
    "    return objects\n",
    "\n",
    "def convert_locale_to_float(df):\n",
    "    loc_float = get_object_columns(df)\n",
    "    converted = df.copy()\n",
    "    for c in loc_float:\n",
    "        converted.loc[:,c] = df[c].apply(get_float)\n",
    "    return converted\n",
    "        \n",
    "def fill_with_mean(train, test):\n",
    "    means=train.mean()\n",
    "    norm_train = train.fillna(means)\n",
    "    norm_test = test.fillna(means)\n",
    "    return norm_train, norm_test\n",
    "\n",
    "# now we have clear non-normalized data, let's normalize first\n",
    "def normalize(train, test):\n",
    "    scaler = StandardScaler()\n",
    "    norm_train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index = train.index)\n",
    "    norm_test = pd.DataFrame(scaler.transform(test), columns=test.columns, index = test.index)\n",
    "    return norm_train, norm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_transform_pipeline(train, test):\n",
    "    train_f = convert_locale_to_float(train)\n",
    "    test_f = convert_locale_to_float(test)\n",
    "    train_cont, test_cont = fill_with_mean(train_f, test_f)\n",
    "    train_cont, test_cont = normalize(train_cont, test_cont)\n",
    "    print(train_cont.isnull().values.any() or test_cont.isnull().values.any())\n",
    "    \n",
    "    print(train_cont.shape)\n",
    "    print(test_cont.shape)\n",
    "    return train_cont, test_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cats_to_labels(train_cat, test_cat):\n",
    "    transformed_df = train_cat.copy()\n",
    "    trans_test = test_cat.copy()\n",
    "    for c in train_cat.columns:\n",
    "        encoder = LabelEncoder()\n",
    "        column_train = train_cat[c].astype(str)\n",
    "        column_test = test_cat[c].astype(str)\n",
    "        combined = pd.concat([column_train, column_test])\n",
    "        encoder.fit(combined)\n",
    "        transformed_df[c] = encoder.transform(column_train).reshape(-1,1)\n",
    "        trans_test[c] = encoder.transform(column_test).reshape(-1,1)\n",
    "    return transformed_df, trans_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_transform_pipeline(train, test):\n",
    "    train_cat, test_cat = transform_cats_to_labels(train, test)\n",
    "    print(train_cat.shape)\n",
    "    print(test_cat.shape)\n",
    "    return train_cat, test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_non_targeted(train_array, y_train):\n",
    "    clean_array = []\n",
    "    train_array.append(y_train)\n",
    "    #clear nans in target\n",
    "    indexes_to_delete = y_train[y_train.isnull()].index\n",
    "    for df in train_array:\n",
    "        item = df.drop(index=indexes_to_delete)\n",
    "        clean_array.append(item)\n",
    "        print(item.shape)\n",
    "    return clean_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_for_cats(train, test, y):\n",
    "    cb_regressor = CatBoostRegressor(logging_level=\"Silent\")\n",
    "    train_catboost_preds = cross_val_predict(cb_regressor, train, y)\n",
    "    cb_regressor.fit(train,y=y)\n",
    "    test_catboost_preds = pd.Series(cb_regressor.predict(test), index=test.index)\n",
    "    return train_catboost_preds, test_catboost_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cont_ensemble():\n",
    "    ridge = Ridge()\n",
    "    rtree = RandomForestRegressor(n_jobs=-1, n_estimators=50)\n",
    "    svr = LinearSVR()\n",
    "    return [ridge, rtree, svr]\n",
    "\n",
    "def get_cont_ensemble_names():\n",
    "    return [\"ridge\", \"rtree\", \"svr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_train_preds(X, y, train_mixture, mix_cols):\n",
    "    predicts = []\n",
    "    for cl in get_cont_ensemble():\n",
    "        predicts.append(cross_val_predict(cl, X,y, n_jobs=-1))\n",
    "    predicts.append(train_mixture)\n",
    "    return pd.DataFrame(np.vstack(predicts).transpose(), index=y.index, columns=get_cont_ensemble_names()+mix_cols)\n",
    "\n",
    "def get_meta_test_predict(X_train, y_train, X_test, test_mixture, mix_cols):\n",
    "    test_predicts = []\n",
    "    for cl in get_cont_ensemble():\n",
    "        print(cross_val_score(cl, X_train, y_train, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "        cl.fit(X_train, y_train)\n",
    "        pr = cl.predict(X_test)\n",
    "        test_predicts.append(pr)\n",
    "    test_predicts.append(test_mixture)\n",
    "    return  pd.DataFrame(np.vstack(test_predicts).transpose(), index=X_test.index, columns=get_cont_ensemble_names()+mix_cols)\n",
    "\n",
    "def get_stacked_ensemble_predict(X_meta, y, X_test):\n",
    "    regressor = xgb.XGBRegressor()\n",
    "    regressor.fit(X_meta, y)\n",
    "    return pd.DataFrame(regressor.predict(X_test), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_item_index(group_size, df, group):\n",
    "    new_df = pd.concat([df, group], axis = 1)\n",
    "    index = []\n",
    "    group = new_df.groupby([\"Скважина\"])\n",
    "    for name, group in group:\n",
    "        if len(group)<group_size:\n",
    "            continue\n",
    "        for start in range(len(group.index) - group_size):\n",
    "            gr =group.index[start:start+group_size]\n",
    "            index.append(gr)\n",
    "    return index\n",
    "\n",
    "def get_timed_ds(meta_size, df, group, y):\n",
    "    if meta_size >= 1:\n",
    "        meta_indexes = get_n_item_index(meta_size, df, group)\n",
    "        first_value_idx = []\n",
    "        timed_ds = df.copy()\n",
    "        metas = []\n",
    "        columns = []\n",
    "        for i in range(meta_size):\n",
    "            columns.append(\"meta%s\"%str(i))\n",
    "        for a in meta_indexes:\n",
    "            first_value_idx.append(a[0])\n",
    "            metas.append(list(y.loc[a]))\n",
    "        metas_df = pd.DataFrame.from_records(metas, index=first_value_idx, columns=columns)\n",
    "        return pd.concat([timed_ds.loc[first_value_idx], metas_df], axis=1)\n",
    "    elif meta_size == 0:\n",
    "        return df\n",
    "    \n",
    "def get_n_tower_predictions(n, train, y, test, train_group, train_mix, test_mix, mix_col):\n",
    "    X_meta_train = get_meta_train_preds(train, y, train_mixture=train_mix, mix_cols=mix_col)\n",
    "    X_meta_test = get_meta_test_predict(train, y, test, test_mixture=test_mix, mix_cols=mix_col)\n",
    "    test_predictions= []\n",
    "    for i in range(0,n):\n",
    "        train_timed_ds = get_timed_ds(i,X_meta_train, train_group, y)\n",
    "        y_timed = y.loc[train_timed_ds.index]\n",
    "        test_predict = get_stacked_ensemble_predict(train_timed_ds, y_timed,X_meta_test)\n",
    "        test_predictions.append(test_predict)\n",
    "        X_meta_test[\"meta%s\"%i]=test_predict\n",
    "    return pd.concat(test_predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(fname, df):\n",
    "    final_pred = pd.Series(final_pred)\n",
    "    final_pred.to_csv(fname,header=[\"_VAL_\"],index_label=[\"_ID_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(train, test):\n",
    "    train, test, y_train  = common_data_pipeline(train, test)\n",
    "    train_group, train_cont, train_dat, train_cat, train_text = split_continious_date_categorical_text(train)\n",
    "    test_group, test_cont, test_dat, test_cat, test_text = split_continious_date_categorical_text(test)\n",
    "    train_cont, test_cont = cont_transform_pipeline(train_cont, test_cont)\n",
    "    train_cat, test_cat = cat_transform_pipeline(train_cat, test_cat)\n",
    "    train_cont, train_group, train_cat, y_train = clean_non_targeted([train_cont, train_group, train_cat], y_train)\n",
    "    train_cat_preds, test_cat_preds = get_preds_for_cats(train_cat, test_cat, y_train)\n",
    "    return train_cont, y_train, test_cont, train_group, train_cat_preds, test_cat_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(train, test, constant = 701.4750):\n",
    "    train_cont, y_train, test_cont, train_group, train_cat_preds, test_cat_preds = get_clean_data(train,test)\n",
    "    time_serie_pred = get_n_tower_predictions(\n",
    "        6, train_cont, y_train,test_cont, train_group, train_cat_preds, test_cat_preds, [\"catboost\"]\n",
    "    ).values.reshape(-1,1)\n",
    "    time_serie_pred = np.squeeze(time_serie_pred)\n",
    "    print(constant - np.mean(time_serie_pred))\n",
    "    final_pred = time_serie_pred + (constant - np.mean(time_serie_pred))\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5735, 147)\n",
      "(319, 138)\n",
      "(5735, 147)\n",
      "(319, 138)\n",
      "{'Агент закачки', 'Станок-качалка', 'Фирма ГРП', 'Тип газосепаратора', 'Примечание'} dropped\n",
      "Нефть, т dropped\n",
      "['ТП(ИДН) Дебит жидкости скорр-ый', 'ТП(ГРП) Дебит жидкости скорр-ый', 'ТП(ГРП) Дебит жидкости', 'ГП - Общий прирост Qн', 'Дебит жидкости', 'ТП(ИДН) Дебит жидкости', 'Жидкость, м3', 'Нефть, м3'] dropped\n",
      "(5735, 133)\n",
      "(319, 133)\n",
      "False\n",
      "(5735, 83)\n",
      "(319, 83)\n",
      "(5735, 37)\n",
      "(319, 37)\n",
      "(4764, 83)\n",
      "(4764, 1)\n",
      "(4764, 37)\n",
      "(4764,)\n",
      "[-124.54067161 -130.03049227 -137.52117274]\n",
      "[-49.1389539  -52.8103233  -61.15671562]\n",
      "[ -90.28929007 -121.49943394 -110.63281796]\n",
      "458.4420562744141\n"
     ]
    }
   ],
   "source": [
    "preds = get_prediction(get_train(), get_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(\"all_data_sub.csv\", preds,constant=701.4750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_test(test, size=6):\n",
    "    test, y_test = get_target(test)\n",
    "    test, y_test = clean_non_targeted([test], y_test)\n",
    "    X_idx = []\n",
    "    y_idx = []\n",
    "    group = test.groupby([\"Скважина\"])\n",
    "    for name, group in group:\n",
    "        if len(group)!= size:\n",
    "            continue\n",
    "        X_idx.append(group.index[0])\n",
    "        y_idx.extend(group.index)\n",
    "    if X_idx and y_idx:\n",
    "        test = test.loc[X_idx]\n",
    "        y_test = y_test[y_idx]\n",
    "        constant = np.mean(y_test.values)\n",
    "        return test, y_test, constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(train, n_folds = 2):\n",
    "    errors = []\n",
    "    k_fold = KFold(n_splits=n_folds)\n",
    "    for tr_ix, test_ix in k_fold.split(train):\n",
    "        print(\"new fold started\")\n",
    "        train_cv = train.iloc[tr_ix]\n",
    "        test_cv = train.iloc[test_ix]\n",
    "        test_cv, y_test, constant = get_cleaned_test(test_cv)\n",
    "        test_preds = get_prediction(train_cv, test_cv, constant=constant)\n",
    "        error=mean_absolute_error(y_true=y_test, y_pred=test_preds)\n",
    "        print(\"fold error:%s\" % error)\n",
    "        errors.append(error)\n",
    "        print(\"overall error: %s\" % np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5735, 147)\n",
      "new fold started\n",
      "Нефть, т dropped\n",
      "(2432, 146)\n",
      "(2432,)\n",
      "(2867, 147)\n",
      "(90, 146)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/env/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Тип газосепаратора', 'Мероприятия', 'КНС', 'Станок-качалка', 'Фирма ГРП', 'Причина простоя.1', 'Примечание'} dropped\n",
      "Нефть, т dropped\n",
      "[] dropped\n",
      "(2867, 139)\n",
      "(90, 139)\n",
      "False\n",
      "(2867, 91)\n",
      "(90, 91)\n",
      "(2867, 37)\n",
      "(90, 37)\n",
      "(2332, 91)\n",
      "(2332, 1)\n",
      "(2332, 37)\n",
      "(2332,)\n",
      "[-4.60681326 -3.56905785 -3.89639142]\n",
      "[-12.11927481  -4.83143372  -6.69613951]\n",
      "[-29.06663199 -23.97000996 -28.0306708 ]\n",
      "-231.75713937717018\n",
      "fold error:318.18226066193756\n",
      "overall error: 318.18226066193756\n",
      "new fold started\n",
      "Нефть, т dropped\n",
      "(2332, 146)\n",
      "(2332,)\n",
      "(2868, 147)\n",
      "(58, 146)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/env/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Мероприятия', 'Станок-качалка', 'Причина простоя.1', 'Фирма ГРП', 'Тип газосепаратора', 'Примечание'} dropped\n",
      "Нефть, т dropped\n",
      "[] dropped\n",
      "(2868, 140)\n",
      "(58, 140)\n",
      "False\n",
      "(2868, 91)\n",
      "(58, 91)\n",
      "(2868, 38)\n",
      "(58, 38)\n",
      "(2432, 91)\n",
      "(2432, 1)\n",
      "(2432, 38)\n",
      "(2432,)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cross_validate(get_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env]",
   "language": "python",
   "name": "conda-env-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
